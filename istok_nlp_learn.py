import spacy
from typing import Dict, Any, List, Optional
from datetime import datetime
from spacy.training import Example
from spacy.util import minibatch, compounding
import random
from pathlib import Path


class IIoTAnalyzer:
    def __init__(self, model_path: Optional[str] = None):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –º–æ–¥–µ–ª–∏
        if model_path and Path(model_path).exists():
            self.nlp = spacy.load(model_path)
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è NER –º–æ–¥–µ–ª—å")
        else:
            self.nlp = spacy.blank("ru")
            print("üÜï –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è NER –º–æ–¥–µ–ª—å")

        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–µ–π
        self.equipment_types = {
            "—Å—Ç–∞–Ω–æ–∫": "–°—Ç–∞–Ω–æ–∫", "–ø—Ä–µ—Å—Å": "–ü—Ä–µ—Å—Å", "—Ä–æ–±–æ—Ç": "–†–æ–±–æ—Ç",
            "–∫–æ–Ω–≤–µ–π–µ—Ä": "–ö–æ–Ω–≤–µ–π–µ—Ä", "–∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä": "–ö–æ–º–ø—Ä–µ—Å—Å–æ—Ä"
        }

        self.components = {
            "—à–ø–∏–Ω–¥–µ–ª—å": "–®–ø–∏–Ω–¥–µ–ª—å", "–ø–æ–¥—à–∏–ø–Ω–∏–∫": "–ü–æ–¥—à–∏–ø–Ω–∏–∫",
            "–≥–∏–¥—Ä–∞–≤–ª–∏–∫–∞": "–ì–∏–¥—Ä–∞–≤–ª–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞", "—ç–ª–µ–∫—Ç—Ä–æ–¥–≤–∏–≥–∞—Ç–µ–ª—å": "–≠–ª–µ–∫—Ç—Ä–æ–¥–≤–∏–≥–∞—Ç–µ–ª—å",
            "—Ä–µ–º–µ–Ω—å": "–†–µ–º–µ–Ω—å", "–ø–∞–Ω–µ–ª—å": "–ü–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è"
        }

        self.symptoms = {
            "—à—É–º": "–ù–µ—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–π —à—É–º", "–≤–∏–±—Ä–∞—Ü–∏—è": "–í–∏–±—Ä–∞—Ü–∏—è",
            "–ø–µ—Ä–µ–≥—Ä–µ–≤": "–ü–µ—Ä–µ–≥—Ä–µ–≤", "—Ç–µ—á—å": "–¢–µ—á—å –∂–∏–¥–∫–æ—Å—Ç–∏",
            "–æ—à–∏–±–∫–∞": "–ö–æ–¥ –æ—à–∏–±–∫–∏", "–∑–∞–∫–ª–∏–Ω–∏–≤–∞–Ω–∏–µ": "–ó–∞–∫–ª–∏–Ω–∏–≤–∞–Ω–∏–µ",
            "—Ä–µ–º–æ–Ω—Ç": "–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–º–æ–Ω—Ç", "–æ—Å—Ç–∞–Ω–æ–≤–∫–∞": "–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ä–∞–±–æ—Ç—ã"
        }

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è NER
        if "ner" not in self.nlp.pipe_names:
            self.ner = self.nlp.add_pipe("ner")
        else:
            self.ner = self.nlp.get_pipe("ner")

    def train_ner_model(self, train_data: List[tuple], output_dir: str = "iiot_ner_model", n_iter: int = 30):
        """–û–±—É—á–µ–Ω–∏–µ NER –º–æ–¥–µ–ª–∏"""
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π
        for _, annotations in train_data:
            for ent in annotations.get("entities", []):
                self.ner.add_label(ent[2])

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
        examples = []
        for text, annots in train_data:
            doc = self.nlp.make_doc(text)
            example = Example.from_dict(doc, annots)
            examples.append(example)

        # –û–±—É—á–µ–Ω–∏–µ
        print("üîÑ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è NER –º–æ–¥–µ–ª–∏...")
        optimizer = self.nlp.begin_training()

        for itn in range(n_iter):
            random.shuffle(examples)
            losses = {}
            batches = minibatch(examples, size=compounding(2.0, 16.0, 1.1))
            for batch in batches:
                self.nlp.update(batch, drop=0.3, losses=losses, sgd=optimizer)
            print(f"‚è≥ –ò—Ç–µ—Ä–∞—Ü–∏—è {itn + 1}, –ü–æ—Ç–µ—Ä–∏: {losses.get('ner', 0):.3f}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.nlp.to_disk(output_dir)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{output_dir}'")

    def analyze_text(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–µ–π"""
        doc = self.nlp(text)

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã NER
        ner_result = {
            "equipment": [],
            "equipment_id": [],
            "dates": [],
            "error_codes": [],
            "times": []
        }

        for ent in doc.ents:
            if ent.label_ == "EQUIPMENT":
                ner_result["equipment"].append(ent.text)
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å ID –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
                id_part = ''.join(c for c in ent.text if c.isdigit())
                if id_part:
                    ner_result["equipment_id"].append(id_part)
            elif ent.label_ == "DATE":
                ner_result["dates"].append(ent.text)
            elif ent.label_ == "ERROR_CODE":
                ner_result["error_codes"].append(ent.text)
            elif ent.label_ == "TIME":
                ner_result["times"].append(ent.text)

        # –ê–Ω–∞–ª–∏–∑ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–µ–π
        failure_result = {
            "equipment_type": self._get_equipment_type(doc),
            "components": self._get_components(doc),
            "symptoms": self._get_symptoms(doc),
            "urgency": self._detect_urgency(doc),
            "timestamp": self._get_timestamp(doc)
        }

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        return {
            "ner": ner_result,
            "failure_analysis": failure_result,
            "raw_text": text
        }

    def _get_equipment_type(self, doc) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è"""
        for token in doc:
            if token.lemma_ in self.equipment_types:
                return self.equipment_types[token.lemma_]
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ"

    def _get_components(self, doc) -> List[str]:
        """–ü–æ–∏—Å–∫ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        found = []
        for token in doc:
            if token.lemma_ in self.components:
                found.append(self.components[token.lemma_])
        return found or ["–ù–µ —É–∫–∞–∑–∞–Ω"]

    def _get_symptoms(self, doc) -> List[str]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∏–º–ø—Ç–æ–º–æ–≤"""
        symptoms = []
        text_lower = doc.text.lower()

        for token in doc:
            if token.lemma_ in self.symptoms:
                symptoms.append(self.symptoms[token.lemma_])

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        if "–Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è" in text_lower or "–Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è" in text_lower:
            symptoms.append("–ù–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è")
        if "—Å—Ä–æ—á–Ω—ã–π —Ä–µ–º–æ–Ω—Ç" in text_lower:
            symptoms.append("–¢—Ä–µ–±—É–µ—Ç—Å—è —Å—Ä–æ—á–Ω—ã–π —Ä–µ–º–æ–Ω—Ç")

        return symptoms or ["–°–∏–º–ø—Ç–æ–º—ã –Ω–µ –æ–ø–∏—Å–∞–Ω—ã"]

    def _get_timestamp(self, doc) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏"""
        for ent in doc.ents:
            if ent.label_ == "DATE":
                return ent.text
        return datetime.now().strftime("%d.%m.%Y %H:%M")

    def _detect_urgency(self, doc) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏"""
        text_lower = doc.text.lower()
        urgency_words = {
            "–≤—ã—Å–æ–∫–∞—è": ["—Å—Ä–æ—á–Ω–æ", "–∞–≤–∞—Ä–∏—è", "–æ—Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è", "–∫—Ä–∏—Ç–∏—á–Ω", "—Å—Ä–æ—á–Ω—ã–π"],
            "–Ω–∏–∑–∫–∞—è": ["–Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω", "–ø–ª–∞–Ω–æ–≤—ã–π", "–Ω–µ —Å—Ä–æ—á–Ω–æ"]
        }

        for level, words in urgency_words.items():
            if any(word in text_lower for word in words):
                return level.capitalize()
        return "–°—Ä–µ–¥–Ω—è—è"

    def pretty_print_analysis(self, analysis: Dict[str, Any]):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        print(f"\nüìã –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: '{analysis['raw_text']}'")
        print("=" * 60)

        print("\nüîç –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:")
        for key, values in analysis["ner"].items():
            if values:
                print(f"- {key.upper()}: {', '.join(values)}")

        print("\n‚öôÔ∏è –ê–Ω–∞–ª–∏–∑ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–µ–π:")
        failure = analysis["failure_analysis"]
        print(f"- –¢–∏–ø –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è: {failure['equipment_type']}")
        print(f"- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {', '.join(failure['components'])}")
        print(f"- –°–∏–º–ø—Ç–æ–º—ã: {', '.join(failure['symptoms'])}")
        print(f"- –°—Ä–æ—á–Ω–æ—Å—Ç—å: {failure['urgency']}")
        print(f"- –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {failure['timestamp']}")
        print("=" * 60)


# –ü—Ä–∏–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
TRAIN_DATA = [
    ("–ü–æ–∫–∞–∂–∏ –æ—Ç—á–µ—Ç –ø–æ —Å—Ç–∞–Ω–∫—É 5 –∑–∞ –∏—é–Ω—å 2023 –≥–æ–¥–∞", {
        "entities": [(17, 24, "EQUIPMENT"), (28, 41, "DATE")]
    }),
    ("–ì—Ä–∞—Ñ–∏–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞–Ω–∫–∞ 3 –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –Ω–µ–¥–µ–ª–∏", {
        "entities": [(16, 23, "EQUIPMENT"), (27, 44, "DATE")]
    }),
    ("–û—à–∏–±–∫–∞ E15 –Ω–∞ —Å—Ç–∞–Ω–∫–µ 5 –≤ 10:30", {
        "entities": [(7, 10, "ERROR_CODE"), (15, 22, "EQUIPMENT"), (26, 31, "TIME")]
    }),
]


def main():
    print("=" * 60)
    print("üöÄ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è IIoT.Istok")
    print("=" * 60)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = IIoTAnalyzer()

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    if not Path("iiot_ner_model").exists():
        print("\nüõ† –û–±—É—á–µ–Ω–∏–µ NER –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        analyzer.train_ner_model(TRAIN_DATA)
        analyzer = IIoTAnalyzer("iiot_ner_model")  # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å

    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    while True:
        print("\n–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):")
        user_input = input("> ").strip()

        if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
            break

        if user_input:
            # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
            analysis = analyzer.analyze_text(user_input)
            analyzer.pretty_print_analysis(analysis)

            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API IIoT.Istok
            # –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ equipment_id –∏ –¥–∞—Ç–µ


if __name__ == "__main__":
    main()